텍스트 분석 머신러닝
머신러닝 수행 절차 :
1. 분석목표 명확히하고, 문제 정의, 데이터 수집 방법, 어떤 유형의 모델이 분석에 필요한지 결정
2. 데이터 수집  (해결해야 할 문제에 맞는 다양한 속성의 데이터를 수집 + 정답(label)
3. 데이터 전처리 
4. 특성 추출 또는 특성 선택 (ex: 빈도수 기반으로 단어 집합, 딥러닝과의 차이점)
5. 데이터 분할 ( 학습/테스트 , 학습(검증)/테스트)    from sklearn.model_selection import train_test_split
6. 모델 선택
7. 모델을 데이터 학습을 통해서 모델 적합시킴     model.fit(X_train, y_train)
8. 모델 평가(모델 성능  평가)                        
9. 모델 배표 및 성능향상 (하이퍼파라미터 조정 ..)

BOW는 빈도수 기반으로 분석해야 할 문서들의 벡터화( DTM)
           빈도수 * 단어 출현 문서의 역수 (tfidf)

머신러닝 분류 알고리즘
로지스틱 회귀 (Logistic Regression) : 특성들의 선형 결합을 사용하여 이진 분류 문제에 대한 확률을 예측합니다. 시그모이드 함수를 통과시켜 확률 값을 얻습니다
결정 트리 (Decision Trees) :   각 노드에서는 특정 특성에 대한 테스트를 수행하고, 가지를 통해 다음 노드로 이동합니다
랜덤 포레스트 (Random Forest) : 앙상블,  여러 결정 트리를 생성 (각 트리의 학습 데이터는 부트스트랩으로 추출)
서포트 벡터 머신 (Support Vector Machines, SVM) : 데이터를 가장 잘 나눌 수 있는 초평면(hyperplane)을 찾아 분류하는 알고리즘,  여러 클래스의 경계면을 찾을 때 커널함수를 사용 => 비선형분류에 성능 좋음
K-최근접 이웃 (K-Nearest Neighbors, KNN) :  k개의 훈련 데이터 포인트를 찾아 다수결 투표를 통해 예측을 수행
나이브 베이즈 (Naive Bayes) : 통계 기반의 베이즈이론 기반 , 모든 특성은 독립적이라는 가정, 
                            클래스의 사전 확률과 조건부 확률을 사용해서 예측  (텍스트 분류에 성능이 좋은편)
신경망 (Neural Networks) :  뇌의 신경 구조를 모방한 모델, 사이킷런의 SLP, MLP 

Gradient Boosting Machine
XGB
Light GBM

트랜스포머 (NLP 분야, 언어모델)
BERT (트랜스포머 구조를 사용하여 사전 훈련된 언어 모델)
GPT-3

분류분석 평가 지표 :  오차 행렬
Accuracy :  정확하게 분류된 샘플의 비율,  (정확하게 분류된 샘플 수)/(전체 샘플 수)
정밀도 (Precision) : 양성으로 예측한 샘플 중에서 실제로 양성인 샘플의 수     TP/ (TP+FP) , 암진단 
재현율(recall, Sensitivity) : 실제 양성중에서 정확하게 양성으로 예측한 비율   TP/ (TP+FN) , 암진단 
F1 Score : 정밀도와 재현율의 조화 평균

모델 성능 개선 방법 : .....

#딥러닝으로 와인분류(이진분류)______________________________________________________________________________
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping

import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

# seed 값 설정
seed = 0
numpy.random.seed(seed)
tf.random.set_seed(seed)

# 데이터 입력
df_pre = pd.read_csv('./wine.csv', header=None)
print(df_pre.info())
print(df_pre.head())

# 데이터를 무작위로 섞은 후, 데이터프레임을 넘파이 어레이로 변환
df = df_pre.sample(frac=1)  
dataset = df.values

X=dataset[:, 0:12]
y=dataset[:, 12]

print(X.shape)
print(y.shape)

# 모델 설정
model = Sequential()
model.add(Dense(30,  input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

#모델 컴파일
model.compile(loss='binary_crossentropy',           optimizer='adam',           metrics=['accuracy'])

# 모델 실행
model.fit(X, y, epochs=200, batch_size=200)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))


#딥러닝으로 다중 클래스 분류 (iris) #########################################################
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# 학습데이터 랜덤하게 추출해서 학습 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.
np.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
from sklearn.datasets import load_iris
iris = load_iris()
data = iris.data
target = iris.target
target_names = iris.target_names
 
df = pd.DataFrame(data, columns=iris.feature_names)
df['species'] = target_names[target]
print(df.info())
print(df.head())
 

# 그래프로 확인 (특성값들간의 차이와 관계를 빠르게 파악할 수 있는 pairplot사용 ) : EDA
sns.pairplot(df, hue='species', markers=["o", "s", "D"]) #산점도와 히스토그램,  hue는 카테고리 변수 설정 파라미터
plt.show()

#print(df['species'])
le = LabelEncoder()
df['species'] = le.fit_transform(df['species'])   # [0, 1, 2] 변환

# 데이터 분할
from keras.utils import to_categorical

X = df.iloc[:, :4].values
y = df['species'].values
y = to_categorical(y)  # One-Hot Encoding 
 
# 모델의 설정
model = Sequential()
model.add(Dense(16,  input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))

# 모델 컴파일
model.compile(loss='categorical_crossentropy',            optimizer='adam',            metrics=['accuracy'])

# 모델 실행
model.fit(X, y , epochs=50, batch_size=1)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, y )[1]))

############################################
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_20newsgroups
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

newsdata = fetch_20newsgroups(subset = 'train')  #20개의 다른 주제를 가진 18,846개의 뉴스 그룹 이메일 데이터
print(newsdata.keys())
print('훈련용 샘플의 개수 : {}'.format(len(newsdata.data)))
print('총 주제의 개수 : {}'.format(len(newsdata.target_names)))
print(newsdata.target_names)
print('첫번째 샘플의 레이블 : {}'.format(newsdata.target[0]))

print('7번 레이블이 의미하는 주제 : {}'.format(newsdata.target_names[7]))
print(newsdata.data[0]) # 첫번째 샘플 출력

data = pd.DataFrame(newsdata.data, columns = ['email'])  #학습데이터셋을 email컬럼명으로 DataFrame 구조로 생성
data['target'] = pd.Series(newsdata.target)  # 토픽 카테고리 label을  DataFrame 구조에 target 열로 추가
print(data.info())
print(data.head())

#결측치 데이터 확인
print(data.isnull().values.any())

#email data 중복 개수와 target값의 중복 개수 확인
print('중복을 제외한 샘플의 수 : {}'.format(data['email'].nunique()))
print('중복을 제외한 주제의 수 : {}'.format(data['target'].nunique()))

#20개의 주제별 email 수를 pandas는 plot으로 시각화
data['target'].value_counts().plot(kind='bar'); 

#20개의 주제별 email 개수 출력
print(data.groupby('target').size().reset_index(name='count'))


#테스트 데이터셋 로드
newsdata_test = fetch_20newsgroups(subset='test', shuffle=True)

train_email = data['email']           # X_train = data['email']
train_label = data['target']           # y_train = data['target']
test_email = newsdata_test.data
test_label = newsdata_test.target

vocab_size = 10000  		#학습에서 사용할 최대 단어(특성집합) 개수를 정의하는 변수
num_classes = 20


def prepare_data(train_data, test_data, mode): 	# 전처리 함수
    tokenizer = Tokenizer(num_words = vocab_size) 	# vocab_size 개수만큼의 단어만 사용한다.
    tokenizer.fit_on_texts(train_data)
    X_train = tokenizer.texts_to_matrix(train_data, mode=mode) 	# 샘플 수 × vocab_size 크기의 행렬 생성
    X_test = tokenizer.texts_to_matrix(test_data, mode=mode) 	# 샘플 수 × vocab_size 크기의 행렬 생성
    return X_train, X_test, tokenizer.index_word

X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary') # binary 모드로 변환

y_train = to_categorical(train_label, num_classes) # topic label값을 원-핫 인코딩으로 변환
y_test = to_categorical(test_label, num_classes) # topic label값을 원-핫 인코딩으로 변환

print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))
print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))
print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))
print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))

print('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))
print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999])) 


from tensorflow.keras.layers import Dropout

def fit_and_evaluate(X_train, y_train, X_test, y_test):
    model = Sequential()   
    model.add(Dense(256, input_shape=(vocab_size,), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)
    score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)
    return score[1]

modes = ['binary', 'count', 'tfidf', 'freq'] # 4개의 모드를 리스트에 저장.
# 'binary' 모드 : 단어가 존재하는 경우 1, 그렇지 않은 경우 0으로 이진화된 행렬을 생성
# 'count' 모드 : 각 문서에서 단어가 나타난 빈도를 기반으로 행렬을 생성
# 'tfidf' 모드 :  각 단어의 가중치를 계산하여 문서 집합에 대한 중요도를 반영 (각 문서에서 단어 빈도 * 역 문서 빈도)
# 'freq' 모드 :  각 문서에서 단어의 상대적인 빈도를 기반으로 행렬을 생성

for mode in modes:
    X_train, X_test, _ = prepare_data(train_email, test_email, mode)
    score = fit_and_evaluate(X_train, y_train, X_test, y_test) 
    print(mode+' 모드의 테스트 정확도:', score)


#####################################################################################
import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews
fileids = movie_reviews.fileids()
reviews = [movie_reviews.raw(fileid) for fileid in fileids]  
categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] 

print('Reviews count:', len(reviews))
print('Length of the first review:', len(reviews[0]))
print('Labels:', set(categories))

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
np.random.seed(7)
tf.random.set_seed(7)

max_words = 10000
tokenizer = Tokenizer(num_words=max_words, oov_token='UNK') 
tokenizer.fit_on_texts(reviews)  #빈도가 높은 단어 10000개 선택

X = tokenizer.texts_to_sequences(reviews)   #빈도가 높은 단어 10000개 기준으로 순서를 가지는 데이터 변환

print('Lengths of first 10 documents:', [len(doc) for doc in X[:10]])

print("Index of 'the':", tokenizer.word_index["the"])
print("Index of 'review':", tokenizer.word_index["review"])
print("Index of out-of-vocabulary words:", tokenizer.word_index["UNK"])

maxlen = 500 #각 리뷰 원문에서 단어수를 500제한

from tensorflow.keras.preprocessing.sequence import pad_sequences
X = pad_sequences(X, maxlen=maxlen, truncating='pre') 

import numpy as np

# label을 0, 1의 값으로 변환
label_dict = {'pos':1, 'neg':0}
y = np.array([label_dict[c] for c in categories])
print(set(y))
   
#학습, 테스트 셋으로 분리 (8:2)
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)

print('Train set count:', len(X_train))
print('Test set count:', len(X_test))
print('Test samples:', y_test[:20])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Embedding

#one-hot encoding -> size 32 밀집 벡터
model = Sequential([ Embedding(max_words, 32, input_length=maxlen), Flatten(), Dense(1, activation='sigmoid')])
model.summary()

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])

history = model.fit(X_train, y_train,  epochs=10,  verbose=1,  validation_split=0.2)

score = model.evaluate(X_test, y_test)
print(f'#Test accuracy:{score[1]:.3f}')

#### 문맥의 순서 정보를 활용한 RNN

from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.optimizers import Adam

#Embedding ->RNN ->은닉층 -> 출력층
model = Sequential([ Embedding(max_words, 32),  SimpleRNN(32) , Dense(32, activation='relu'), Dense(1, activation='sigmoid')])
model.summary()

adam = Adam(learning_rate=1e-4)
model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train,  epochs=10,  verbose=1,  validation_split=0.2)

score = model.evaluate(X_test, y_test)
print(f'#Test accuracy:{score[1]:.3f}')

#워드 임베딩(Word Embedding) :  
단어를  one-hot벡터로 변환해서 신경망으로 학습하고 문맥 정보를 담은 밀집 벡터로 변환하는 것
LSA, Word2Vec, FastText, Glove

1. 각 단어에 인덱싱 => 어휘사전
2. 각 단어의 인덱스에 대응하는 임베딩 벡터를 저장하는 테이블 생성
   가중치 초기화 -> 신경망 학습-> 벡터  표현 
3. 입력 텍스트의 각 단어를 인덱스 변환 >  임베딩 테이블에서 단어에 대한 임베딩 벡터를 가져옴 > 임베딩 벡터로 변환 > 각 단어의 임베딩 벡터를 연결 , 평균 ,...밀집벡터로 변환

https://wikidocs.net/22660

from tensorflow.keras.layers import LSTM, Bidirectional
model = Sequential([ Embedding(max_words, 64),  Bidirectional(LSTM(64)), Dense(64, activation='relu'), Dense(1, activation='sigmoid')])
model.summary()

adam = Adam(learning_rate=1e-4)
model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train,  epochs=10,  verbose=1,  validation_split=0.2)

score = model.evaluate(X_test, y_test)
print(f'#Test accuracy:{score[1]:.3f}')












